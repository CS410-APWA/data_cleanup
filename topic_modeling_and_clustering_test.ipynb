{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from earthy.nltk_wrappers import lemmatize_sent\n",
    "from earthy.nltk_wrappers import porter_stem\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'essays/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(path)\n",
    "\n",
    "data = {}\n",
    "for filename in filenames:\n",
    "    with open(path + filename, \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        data[filename] = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame(data, index=[1]).transpose()\n",
    "data_df.columns = ['essay']\n",
    "data_df = data_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation) \n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word, 'v') for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "data_clean = pd.DataFrame(data_df.essay.apply(clean))\n",
    "essay_compiled = data_clean.essay.tolist()\n",
    "essay_clean = [essay.split() for essay in essay_compiled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(essay_clean)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=10000)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "corpus = [dictionary.doc2bow(essay) for essay in essay_clean]\n",
    "corpus_df = pd.DataFrame(corpus)\n",
    "\n",
    "#essay_term_df.to_csv('~/Desktop/gensim_vectorized_data.csv')\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "#for essay in tfidf[essay_term_matrix]:\n",
    "#    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in essay])\n",
    "\n",
    "# Topic modeling: LDA w/o tf-idf\n",
    "# Ana's:\n",
    "#lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=15, update_every=1, chunksize=100, random_state=100, passes=1)\n",
    "\n",
    "# Other:\n",
    "#lda = gensim.models.LdaMulticore(corpus=corpus, num_topics=15, id2word=dictionary, passes=2, workers=2, chunksize=100, random_state=100,)\n",
    "\n",
    "\n",
    "# Topic modeling: LDA w/ tf-idf\n",
    "lda_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "\n",
    "# Print topics.\n",
    "#for idx, topic in lda_tfidf.print_topics(-1):\n",
    "#    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9544</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[tne, z, j, hole, experience, rouse, bed, correctional, officer, 9, be, early, me, wednesday, morning, pack, up, transfer, id, wait, list, go, hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[ok, stephen, whetzel, page, 1, 14, van, slice, dense, morning, fog, like, jet, vapor, d1trk, mood, somber, ten, us, twg, guard, eight, prisoners,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[ï»¿jamaal, freeman, alabama, introspection, topic, thoughts, konvict, the, divine, manifestation, god, prisons, place, criminals, confine, break,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[plan, action, come, america, start, work, construction, industry, work, two, years, san, antonio, area, progress, manual, labor, job, attend, col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[worthy, live, write, list, below, âreasons, dismiss, older, people, live, worthless, uselessâ, tongueâinâcheek, manner, inspire, contempt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[satisfaction, mira, judgment, black, clone, ï¬ebt, ï¬eparted, october, 16, 2013, state, missouri, file, gatisfaction, juï¬gment, motion, cole,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6304</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[72, 4, prison, witers, workshop, recently, moveo, exchange, passiveagressive, ole, man, josh, guy, year, younger, myself, also, like, write, weve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[hâgc, 1, h, second, prison, writers, workshop, finish, write, first, story, prison, writers, workshop, twentyâtwo, hundred, word, long, make,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[3, xi, nao3, xi3, wash, ânao3, brain, word, root, derive, chinese, mandarin, english, deï¬nition, brainwash, 1, intensive, usu, political, ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9262</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[mock, bird, endanger, effort, awaken, social, conscience, evils, injustice, intolerance, discrimination, vengeance, hate, author, harper, lee, wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[john, peters, jr, a209244, madison, correctional, institution, 1851, state, route, 56, po, box, 740, london, ohio, 43140, 2008, serve, 20, full, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9023</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[life, without, children, linda, field, ccwf, come, prison, sara, seven, young, understand, 25, life, mean, sheâd, grow, without, mother, brothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9286</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[dear, american, prison, write, archive, another, essay, perusal, sure, receive, before, since, write, much, hopefully, new, you, send, essay, jou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9288</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[ï»¿daniel, scott, gentry, 112014, conform, reform, need, improve, conform, adapt, rule, pattern, custom, etc, reform, form, reconstruct, reclaim,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9221</td>\n",
       "      <td>inmate, officer, cell, im, staff, want, here, guy, work, food</td>\n",
       "      <td>[examine, prison, disciplinehypocrisy, case, study, 3a, punish, visitors, felons, frederick, mason, 55487âo56, usp, tucson, po, box, 24550, tucs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0             0             3.0              0.9544   \n",
       "1             1             3.0              0.9576   \n",
       "2             2             3.0              0.9454   \n",
       "3             3             3.0              0.8915   \n",
       "4             4             3.0              0.9311   \n",
       "5             5             3.0              0.8681   \n",
       "6             6             3.0              0.6304   \n",
       "7             7             3.0              0.8783   \n",
       "8             8             3.0              0.9630   \n",
       "9             9             3.0              0.9262   \n",
       "10           10             3.0              0.9382   \n",
       "11           11             3.0              0.9023   \n",
       "12           12             3.0              0.9286   \n",
       "13           13             3.0              0.9288   \n",
       "14           14             3.0              0.9221   \n",
       "\n",
       "                                                         Keywords  \\\n",
       "0   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "1   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "2   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "3   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "4   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "5   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "6   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "7   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "8   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "9   inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "10  inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "11  inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "12  inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "13  inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "14  inmate, officer, cell, im, staff, want, here, guy, work, food   \n",
       "\n",
       "                                                                                                                                                     Text  \n",
       "0   [tne, z, j, hole, experience, rouse, bed, correctional, officer, 9, be, early, me, wednesday, morning, pack, up, transfer, id, wait, list, go, hol...  \n",
       "1   [ok, stephen, whetzel, page, 1, 14, van, slice, dense, morning, fog, like, jet, vapor, d1trk, mood, somber, ten, us, twg, guard, eight, prisoners,...  \n",
       "2   [ï»¿jamaal, freeman, alabama, introspection, topic, thoughts, konvict, the, divine, manifestation, god, prisons, place, criminals, confine, break,...  \n",
       "3   [plan, action, come, america, start, work, construction, industry, work, two, years, san, antonio, area, progress, manual, labor, job, attend, col...  \n",
       "4   [worthy, live, write, list, below, âreasons, dismiss, older, people, live, worthless, uselessâ, tongueâinâcheek, manner, inspire, contempt...  \n",
       "5   [satisfaction, mira, judgment, black, clone, ï¬ebt, ï¬eparted, october, 16, 2013, state, missouri, file, gatisfaction, juï¬gment, motion, cole,...  \n",
       "6   [72, 4, prison, witers, workshop, recently, moveo, exchange, passiveagressive, ole, man, josh, guy, year, younger, myself, also, like, write, weve...  \n",
       "7   [hâgc, 1, h, second, prison, writers, workshop, finish, write, first, story, prison, writers, workshop, twentyâtwo, hundred, word, long, make,...  \n",
       "8   [3, xi, nao3, xi3, wash, ânao3, brain, word, root, derive, chinese, mandarin, english, deï¬nition, brainwash, 1, intensive, usu, political, ind...  \n",
       "9   [mock, bird, endanger, effort, awaken, social, conscience, evils, injustice, intolerance, discrimination, vengeance, hate, author, harper, lee, wr...  \n",
       "10  [john, peters, jr, a209244, madison, correctional, institution, 1851, state, route, 56, po, box, 740, london, ohio, 43140, 2008, serve, 20, full, ...  \n",
       "11  [life, without, children, linda, field, ccwf, come, prison, sara, seven, young, understand, 25, life, mean, sheâd, grow, without, mother, brothe...  \n",
       "12  [dear, american, prison, write, archive, another, essay, perusal, sure, receive, before, since, write, much, hopefully, new, you, send, essay, jou...  \n",
       "13  [ï»¿daniel, scott, gentry, 112014, conform, reform, need, improve, conform, adapt, rule, pattern, custom, etc, reform, form, reconstruct, reclaim,...  \n",
       "14  [examine, prison, disciplinehypocrisy, case, study, 3a, punish, visitors, felons, frederick, mason, 55487âo56, usp, tucson, po, box, 24550, tucs...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatTopicSentences(ldamodel=lda_tfidf, corpus=corpus_tfidf, texts=essay_clean):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = lda.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = formatTopicSentences(ldamodel=lda_tfidf, corpus=corpus_tfidf, texts=essay_clean)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = '/Users/inesayara/Desktop/senior_seminar/essays/' \n",
    "vectorizer = TfidfVectorizer(encoding=\"ISO-8859-1\", input='filename', stop_words='english')\n",
    "dtm = vectorizer.fit_transform([full_path + filename for filename in filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize w/ CountVectorizer\n",
    "\n",
    "#vectorizer = CountVectorizer(stop_words='english')\n",
    "#vectorized_data = vectorizer.fit_transform(data_clean.essay)\n",
    "#data_dtm = pd.DataFrame(vectorized_data.toarray(), columns=vectorizer.get_feature_names())\n",
    "#data_dtm.to_csv('~/Desktop/sklearn_vectorized_data.csv')\n",
    "#data_dtm.index = data_clean.index\n",
    "#data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'solidarity'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=1000, n_init=1)\n",
    "model.fit(dtm)\n",
    "\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "terms = [lemma.lemmatize(term) for term in terms]\n",
    "terms = \" \".join(w for w in nltk.wordpunct_tokenize(word) if w.lower() in terms or not w.isalpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = dtm.toarray() \n",
    "terms = np.array(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters= 15\n",
    "estimator = KMeans(n_clusters)\n",
    "k = estimator.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 381, 10: 238, 4: 199, 7: 183, 11: 183, 1: 93, 3: 72, 8: 44, 2: 38, 14: 35, 5: 34, 13: 22, 6: 21, 12: 16, 0: 14})\n"
     ]
    }
   ],
   "source": [
    "#count and output how many files/essays per cluster\n",
    "labels = estimator.labels_\n",
    "print (Counter(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " ptsd\n",
      " richard\n",
      " ip\n",
      " brain\n",
      " cdcr\n",
      " bladder\n",
      " stress\n",
      " symptom\n",
      " plaintiff\n",
      " health\n",
      " cystectomy\n",
      " treatment\n",
      " medical\n",
      " disorder\n",
      " prison\n",
      "Cluster 1:\n",
      " inmate\n",
      " prison\n",
      " inmate\n",
      " staff\n",
      " time\n",
      " officer\n",
      " prison\n",
      " correctional\n",
      " like\n",
      " life\n",
      " people\n",
      " don\n",
      " work\n",
      " money\n",
      " year\n",
      "Cluster 2:\n",
      " carolina\n",
      " african\n",
      " north\n",
      " party\n",
      " american\n",
      " vote\n",
      " black\n",
      " worley\n",
      " prison\n",
      " voting\n",
      " willie\n",
      " american\n",
      " political\n",
      " 3rd\n",
      " community\n",
      "Cluster 3:\n",
      " god\n",
      " tucson\n",
      " inmate\n",
      " prison\n",
      " usp\n",
      " officer\n",
      " guy\n",
      " don\n",
      " pause\n",
      " huffstuttler\n",
      " inmate\n",
      " people\n",
      " like\n",
      " officer\n",
      " let\n",
      "Cluster 4:\n",
      " cell\n",
      " prison\n",
      " time\n",
      " like\n",
      " told\n",
      " said\n",
      " just\n",
      " day\n",
      " unit\n",
      " got\n",
      " year\n",
      " prisoner\n",
      " did\n",
      " door\n",
      " jail\n",
      "Cluster 5:\n",
      " parole\n",
      " board\n",
      " ohio\n",
      " release\n",
      " prisoner\n",
      " year\n",
      " law\n",
      " commissioner\n",
      " sentence\n",
      " old\n",
      " offender\n",
      " prison\n",
      " crime\n",
      " hearing\n",
      " released\n",
      "Cluster 6:\n",
      " translated\n",
      " stull\n",
      " loera\n",
      " don\n",
      " school\n",
      " mexico\n",
      " family\n",
      " violence\n",
      " arrived\n",
      " year\n",
      " prison\n",
      " houston\n",
      " want\n",
      " thing\n",
      " going\n",
      "Cluster 7:\n",
      " life\n",
      " prison\n",
      " time\n",
      " ve\n",
      " year\n",
      " just\n",
      " like\n",
      " know\n",
      " people\n",
      " family\n",
      " day\n",
      " don\n",
      " thing\n",
      " way\n",
      " did\n",
      "Cluster 8:\n",
      " solitary\n",
      " confinement\n",
      " cell\n",
      " prisoner\n",
      " mental\n",
      " isolation\n",
      " torture\n",
      " prison\n",
      " year\n",
      " mentally\n",
      " day\n",
      " long\n",
      " term\n",
      " time\n",
      " health\n",
      "Cluster 9:\n",
      " prison\n",
      " life\n",
      " world\n",
      " like\n",
      " people\n",
      " time\n",
      " man\n",
      " men\n",
      " just\n",
      " love\n",
      " black\n",
      " society\n",
      " self\n",
      " don\n",
      " ve\n",
      "Cluster 10:\n",
      " prison\n",
      " prisoner\n",
      " society\n",
      " people\n",
      " crime\n",
      " prison\n",
      " state\n",
      " prisoner\n",
      " year\n",
      " time\n",
      " offender\n",
      " criminal\n",
      " life\n",
      " program\n",
      " justice\n",
      "Cluster 11:\n",
      " court\n",
      " law\n",
      " trial\n",
      " state\n",
      " justice\n",
      " right\n",
      " criminal\n",
      " case\n",
      " police\n",
      " prison\n",
      " government\n",
      " evidence\n",
      " prisoner\n",
      " year\n",
      " conviction\n",
      "Cluster 12:\n",
      " texas\n",
      " tdcj\n",
      " slavery\n",
      " unit\n",
      " beep\n",
      " inmate\n",
      " prison\n",
      " state\n",
      " prison\n",
      " labor\n",
      " woman\n",
      " prisoner\n",
      " kathy\n",
      " slave\n",
      " commissary\n",
      "Cluster 13:\n",
      " te\n",
      " tha\n",
      " ta\n",
      " tn\n",
      " anï\n",
      " tho\n",
      " ef\n",
      " iï\n",
      " fer\n",
      " ha\n",
      " net\n",
      " ma\n",
      " er\n",
      " en\n",
      " cf\n",
      "Cluster 14:\n",
      " death\n",
      " penalty\n",
      " california\n",
      " hartman\n",
      " prison\n",
      " kenneth\n",
      " com\n",
      " life\n",
      " parole\n",
      " prisoner\n",
      " org\n",
      " possibility\n",
      " book\n",
      " execution\n",
      " www\n"
     ]
    }
   ],
   "source": [
    "#output top terms per cluster\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = estimator.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(n_clusters):\n",
    "    print (\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :15]:\n",
    "        print (' %s' % terms[ind]),\n",
    "    print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
